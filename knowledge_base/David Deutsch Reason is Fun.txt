Lulie
Welcome to the Reason is Fun podcast. I'm your host, Lulie Tannett. Today I'm having a conversation with David Deutsch about AGI and epistemology. And it's more of a conversation than an interview, so I get a little bit interrupt-y at times. This episode might be a little rough around the edges, which is why I've called it Episode Minus One. And Episode Zero might and episode zero might also be a bit rough, so I appreciate your patience. And with that, let's get into it. A bunch of things are happening in the world right now regarding AI, and people are panicking, and I wanted to know what you thought about that whole thing.

David Deutsch
Yeah, my broad thought, with which I always reply when people ask me about this is that AI is not the same as AGI AGI is not a Very advanced form of AI. Will AI destroy the universe or just the world? I think there's no more reason to think that AI will destroy the world than any other technology or than people in general. AGI, once we have it, will be just people and they certainly have the potential to destroy the world, but we also have very deep knowledge about how to prevent that. So as long as we keep making progress, I don't think there's anything more effective we can do to ensure good outcomes in the future than to keep making progress. Certainly if we don't make progress, we've guaranteed our doom.

Lulie
Okay, suppose I am writing a reply to a Bayesian who is very worried about AI and is specifically worried that it will grow too fast and then it will gain intelligence and then do all sorts of bad, dangerous things and destroy the world.

David Deutsch
is the way that people are focusing on a particular danger that is worrying them for some reason and they ignore equally dangerous or worse possibilities that either are always around or have been around for a long time. So why do some people freak out, I don't think that's too strong a phrase, over AI risk? And other people freak out over climate change risk. And now people are starting to freak out about nuclear war risk again. That hasn't been happening for like decades now. But I remember when it was the big thing.

Lulie
Did you see a similar freak-out in the last time this happened?

David Deutsch
Absolutely. I mean, I think probably more than now, it was a complete consensus that everybody was afraid. Let me think. I think everybody was appropriately afraid. Was it like the pandemic? It's unlike today.

Lulie
Was it like the pandemic freak-out? Because you also got people actually freaking out around the pandemic and doing all sorts of things like covering their door handles in copper and not knowing whether masks work or not and so on and these battles with their friends about whether

David Deutsch
they leave their house and whether that is akin to killing people. The interesting thing, I think there's an interesting difference between being afraid of something like nuclear war or a pandemic and being afraid of something that one imagines, like AI risk or climate risk. So those are both things that that might happen in the future and different people might imagine different things about them.

Lulie
You mean AGI risk in the future?

David Deutsch
Well there's AGI risk and nowadays there seems to be AI risk as well. I mean there is AI risk. People are using it to scam people and to fake voices of Barack Obama and ring people's grandmothers and so what about those? Yes, well those are real risks and you know there's the electric car risk and the self-driving car risk and so on and that's that's not in the same league as as having a greatly increased ability to scam and get like dodgy information it's interesting is it greatly increased i mean i i wonder whether anyone has statistics about how many scams are currently advanced ai enabled and how many are simply the same old scams of saying hello we're the police we want you to transfer all your money into this account.

Lulie
I imagine the good scams, as in the effective scams, would be AI enabled, like you want to be

David Deutsch
on the leading edge of making scams. Well, I don't know, I'm not an expert on scams, but the thing is dangers, including scams and everything, will always be caused by new technology. I mean, sorry, new technology will always cause dangers, including scams. And to try to mitigate that by preventing new technology, in case it produces new dangers, is much more dangerous than any of the new technologies themselves.

Lulie
What about just slowing it down such that people can adapt? Because like right now we've got something that is going so quickly that people are getting confused like old people if they see an image then they will assume that it's real and whereas if you have time that people are kind of adapt to, ah yes there are these deep fakes and so on.

David Deutsch
Well I'm not sure that time causes better adaptation because if things are happening fast, then also, news stories about how people have been scammed will be seen by your old people, and whereas if we slowed it down so that only one scam occurs every few months, then it might not be news.

Lulie
issue. People are worried that AGI is, you know, maybe next week or just around the corner or in, like they used to say, in a few years, and now that we have these very good language models they say, maybe, like, small number of years, months, like, possibly weeks, and hence the proposed moratorium. So, what is the thing that makes you so chill? Why couldn't it lead to AGI? What's the problem with the idea of emergence? Because, you know, intelligence emerged once. Yes. So emergence isn't magic.

David Deutsch
It is, of course, possible that in the deep ocean a new form of life is emerging at this very moment and it will break the surface weeks from now.

Lulie
Or on Pluto. Did you hear about Pluto? They discovered that there's ice or something and that they've sent another probe but it's going to take eight years for it to come back and find out whether there's life on Pluto.

David Deutsch
I hadn't heard that but there certainly is a possibility of life in various places in the solar system but not, I think, not intelligent life. But, if you're going to say emergence can do unexpected things, then you might as well say it about the deep ocean, because we know less about that than we do about Pluto. So, and there are theories that our form of life began in the deep ocean. So, who knows? Now, I think, in my view, AGI has as little to do with AI as it has with the deep ocean. Both of them you can say, well, it's unexpected, an unexpected thing could happen, AI could lead to AGI, the deep ocean already produced life once, or maybe more than once. But AI is the opposite of AGI. So I think the most immediate way that I have found to illustrate why I think AI and AGI are opposites is that for an AGI there is such a thing as a criterion for how well it's meeting its specification.

Lulie
Before we jump into that, what makes people think that AI is just a less advanced AGI?

David Deutsch
It's because they have the wrong epistemology. Basically, the prevailing epistemology is not Popperian. in many ways, anti-Papyrian, and it's some form of empiricism, inductivism, and the most recent popular form of inductivism is so-called Bayesianism, although I prefer to call it Bayesian epistemology, because a different thing is also called Bayesianism, which is a certain way of treating statistical tests and that kind of thing using Bayes' theorem. But Bayesian epistemology has a much wider application than that and also has become a popular philosophy of knowledge in its own right that doesn't really have much to do with statistical analysis. People make an argument that you should take a Bayesian view about X, Y and Z when they don't mean look at tables of statistics and apply a certain formula. They mean adopt a certain philosophy.

Lulie
view about how AI becomes AGI? Like, why do they think that's a spectrum rather than a binary?

David Deutsch
Because what the most advanced AIs do is that they do this thing that inductivism or Bayesianism a vast amount of data and process it in a way that can give rise to predictive theories. So some people say, you know, the modern chatbots are just predictive text engines and that's a bit unfair, but at the level of epistemology that's what they think is happening and they don't realize that there is anything else. So why is the prevailing view that AGI is an advanced form of AI? And I was saying because under the prevailing epistemology there is no other way of generating knowledge or new theories or new predictions other than generalizing from data. And it so happens that the latest AI technology, chatbot technology, and also chess playing, you know, all the advanced forms of AI, do in fact operate by taking a vast amount of data and distilling from it, essentially a predictive theory.

Lulie
Wait, so does it work by induction?

David Deutsch
No, it doesn't work by induction in the sense that induction is a theory about how knowledge is created. They don't create knowledge.

Lulie
But they do. I can ask you something and it can generate stuff.

David Deutsch
Yeah, well you can look things up in a dictionary.

Lulie
Yeah, but it generates stuff that doesn't exist on a dictionary?

David Deutsch
So does your calculator. So your calculator produces output that's never been produced on earth before. And you can call that creating knowledge if you like, but it's not knowledge in the sense that we want when we say that we want scientific knowledge or we want human type knowledge. What is the difference between a calculator and an AI and the difference between an AI and an AGI or a human those are two distinct differences. One is that a calculator is essentially an automated look-up table. It consists of an algorithm that was programmed in by somebody who knew what the transformation between the inputs and outputs ought to be. Namely, when you press a certain button it multiplies and so on. AI's, modern AI's, essentially construct their algorithm themselves by generalizing a large amount of data. In early ones, this led to kind of embarrassing glitches, like when they identified a heap of rifles as a cat, because somebody worked out what they were doing and how to fool them. But the modern ones use so much data and have been honed by humans so well that they rarely do this. Although I've recently been playing around with the chat GPTs and I find that if you ask it some questions off the beaten track, you can quite easily cause it to do all the old things of either either saying nonsense, contradicting itself, committing howlers, where it says the opposite of known facts and so on. And that's different from human knowledge which is explanatory. Neither the calculator nor the chatbot ever produces a new explanation. You can ask it for an explanation but all it's doing is distilling explanations that already exist and that's the thing that it doesn't do very well either.

Lulie
What is the difference between an explanation and the thing that it does produce?

David Deutsch
If we knew the detailed answer to that we'd know how to make an AGI. basically an explanation accounts for what it's trying to account for, like a physical process or the reason for something, it accounts for that, it accounts for a known thing that is trying to explain in terms of the unseen, unknown reasons behind it, which usually cannot even be seen even in So Brett Hall's favourite example is that we can never see the centre of the sun. We could never go there, any instrument that we send would get destroyed long before it got to the centre of the sun. So the centre of the sun can't be in GPT's training data? As it were, yes, exactly. And the only thing that can be in GPT's training data is what is seen about the Sun, namely its surface.

Lulie
Couldn't GPT derive things about the Sun based on other theories that we have?

David Deutsch
It can deduce things from existing theories, yes. So if you ask it about the centre of the Sun, it will find some existing theory of the Sun and make a deduction from that. So that's not induction, that's deduction. And on the other hand, if there was a mystery about the Sun, like a couple of decades ago...

Lulie
Isn't it induction via deduction? So the induction was all of the training data,

David Deutsch
and then the deduction is taking that data and then forming theories about it? No, because it didn't induce the data. It distilled it into a more compact form and then it can deduce things from that, but those things are only ever as good as the original theories were, probably slightly worse because by compressing the data it slightly some cases it degraded it a lot. A few decades ago, I was going to say recently, but in fact not so recent, it was when I was a graduate student, the big problem in astrophysics was that the sun wasn't producing enough neutrinos. Enough for what?

3
Enough.

David Deutsch
For your breakfast cereal? producing as many as the theory predicted. And this theory was extremely robust because it was also the theory that we used to predict the sun's brightness. And it predicted the sun's brightness extremely well, and also the brightness of other stars and how they change with time and So when they first made neutrino observatories with rather crude neutrino detectors, first they didn't find neutrinos but then they found a few but nowhere near enough. And when they refined it they found that there were only a third as many neutrinos as as predicted by the theory. And so there were all sorts of explanatory theories proposed, which couldn't possibly have been induced from anything, because all the data said was there are too few neutrinos. And of course you can always say that neutrinos have been eaten by a space monster, but generally when we produce scientific theories, we don't just want a new explanation, we want a new explanation that doesn't upset old explanations, that doesn't make them into nonsense, like the space monster theory. So you asked me what the difference is between an explanation and just a predictive theory. And by the way, there is no such thing as a purely predictive theory. They all have some kind of explanation and when you're quote inducing things from data you're always using an old explanation and just piling on some tweaks which don't have an explanation in order to make your supposed new theory. So no one could have induced from the data or lack of data in this case what the explanation was. Because the explanation turned out to have nothing to do with stars, nothing to do with measuring instruments, nothing to do with space. And somebody came up with it. And then it was tested. And passed the test. And now we know. Turns out that completely unbeknownst to anybody, there are three types of neutrinos and they convert from one to the other. So the Sun only produces one of those kinds, but by the time the neutrinos get to us, they've converted like to... The Sun produces neutrino type 1 and then when it's moved a few million miles from the centre of the Sun, it's converted to neutrino 2. And then, after a while, it's converted to neutrino 3. And our detectors can only see neutrino 1.

Lulie
I'm just imagining a whole line of breakfast cereals.

David Deutsch
Is there a breakfast cereal called neutrino?

Lulie
Well, there's lots of, you know, spaghetti-o... No, what is it?

David Deutsch
Oh, neutrin-o's. Right.

6
Ha ha ha!

David Deutsch
Right. The word neutrino began as a joke.

Lulie
So it is a joke already?

David Deutsch
Yeah. It was one of the mid-20th century physicists, I think it was Enrico Fermi, he was just making a joke about a thing that's like a neutron but tiny. So he called it a neutrino.

Lulie
Like nano?

David Deutsch
I think eno as a suffix has a meaning in Italian, I think. So anyway, so somebody thought of the explanation and it couldn't have been induced and that's Brett Hall's favourite example. He may have got it from my favourite example, which is not as good, which is that nobody could have been present at the Big Bang, yet we form theories of the Big Bang and we do not induce them from stuff we see around us which is nothing like the Big Bang.

Lulie
So AIs can't create explanations which means they can't create anything that isn't already in their data set in some way?

David Deutsch
In some way, yes. they can move around parameters so they can you know if you said imagine some new theory about the sun it might I think it would be able to say things like well maybe the sun is twice as big as we think it is and then if you ask it why it might be able to say it might say things which are already explanations of something and which is drawn into that area. Now people will say well that's how humans make new explanations. Well that's how humans make explanations. Make new explanations. Yeah everything is just you know making connections between existing stuff. Yeah well in the sense... There's nothing new under the sun, David. In the sense that must be true, but that is the same kind of, you know, bad explanation of explanations that as it would be if you said, well, all the explanations are phrased in terms of 26 characters and all that make a new explanation is rearranging those characters in a new way. So why is it not that? Letters of the alphabet. Well... Well because it makes a difference whether we make it into a new explanation or not. And most rearrangements of characters are not new explanations. Ah, so you're saying there's an infinite number of connections you could draw

Lulie
between things and only the ones that like actually work actually work?

David Deutsch
Yes, I mean it's not actually infinite, it's exponentially large, but for practical purposes that's the same thing.

Lulie
When you make a new connection between two things, does that then create a new third thing which you can then make connections between? Is it actually true that the theory of knowledge works by making connections between things? Would you say that's an accurate representation?

David Deutsch
Well, again, if I knew the exact answer to that question, I could make an AGI. But I'm fairly sure, I mean, I can't think of an alternative to the process being and mutating existing things. So you can either change a theory slightly in the hope that the rest of it still works or you can make new combinations between things. That's also the two ways that biological mutations can happen in DNA. You can either have a cosmic ray strikes the DNA and changes one base pair or something, or you can have some error in copying results in a bit of DNA that came from somewhere else being stuck into a particular place in the DNA strand.

Lulie
So this sounds like random variation of one thing rather than a new connection between two things?

David Deutsch
Well, no, the second process is in a way a new connection between two things because it, if there's a whole bit of DNA in a different organism or in a different part of the DNA and it gets copied, as it were, into the wrong place in the evolving organism, and that's a way that evolution can take place. Most instances of that just kill the organism but every so often it either leaves the functionality unchanged or makes it better. So what does that have to do with connecting? If a dog gets a piece of DNA from a lobster then that creates a similarity between dogs and lobsters that didn't exist before. And people can, paleogeneticists or whatever they're called, can look at DNA. Bacteria do this a lot. Higher organisms do it less and less because, I think basically because there's less and less chance of you surviving such a thing. For it to be part of evolution, you've got to be able to survive the new adaptation being slightly there, and a bit more there, and a bit more there, because giving a dog lobster claws wouldn't work, because the machinery for controlling the claws and deciding when to use them and so on hasn't been transferred. It would have to evolve. The point is that human new explanations evolve intentionally. They involve randomness at some level, but the business part of creating the new explanation is what happens to the randomness after it's generated and it is changed and further changed intentionally to solve a problem and that is what induction can't possibly do but we don't know what

Lulie
can do it. Induction can't make a change to solve a problem yes but evolution both genetic and evolution of ideas in the mind can?

David Deutsch
Well, no, it can... so every new genetic sequence is produced first and tried out later. Marxism thinks that an organism's environment and its own actions can cause a change in its genome. And that can't happen, and it couldn't happen because it's the same as induction, which also can't happen. So is this also true within a mind? Like you have to make up something before you can tell

Lulie
whether it solves the problem or not?

David Deutsch
Yes, but there's a difference, which is that the making up process isn't purely random.

5
How so?

David Deutsch
For example, a human can do the thing which I just said the dog and the lobster can't do. can think of the solar neutrino problem and can think, could it be that there's a new particle that isn't even a neutrino? And what would that particle... Now that in itself is not the explanation, but it's the germ of an explanation be like made up before you can check whether that's a good question? Yes, but it that making up that involves much less random trial and error than it would that it would take to try all possible variations randomly. So the fact that a person can think of an analogy, you know, some people think that human thinking is all about analogies. So we can take a whole idea from some other place and see if it fits in this place, which it never does immediately, but then you can say, well, how can we change it further so it does fit? So a person who thought, well, maybe there's a wholly new particle involved, at some point that person may say, maybe that whole new particle is just a neutrino, but of a different type. And then they're well on the way to solving. Of course there are many other considerations, but when I say many, it's nothing compared with how many you'd have to check through by random variation, or by trying every possibility or whatever.

Lulie
Why is it not the case that you need to do this random thing by producing the idea first and checking it afterwards?

David Deutsch
Or are you saying that you can do that? Human minds produce conjectures, raw conjectures, much more efficiently than any either systematic search or random search.

Lulie
And we don't know why?

4
We don't know how.

3
Okay, we don't know how.

David Deutsch
So, for example, chess playing engines have to search through billions of times more possibilities than chess grandmasters do.

Lulie
Well, they apparently do?

David Deutsch
No, it's impossible that... well, unless there's something in the hardware of the human brain that we don't know about.

Lulie
What if it's like super parallel?

David Deutsch
Yeah, well, it would have to go through billions of times more processing than we think it can, which would mean it would have to be billions of times more efficient thermodynamically than we think it is. I mean, we don't know. We don't know how the brain works either, let alone...

Lulie
Do we know this for biological evolution, as in, do we know how they, like, biological evolution is as efficient as it is? Because I thought that we couldn't very well program biological evolution.

David Deutsch
Yeah, we can't, but, and that, the analogue of that program, that, of that problem does exist but it's not as severe a problem as it is for thinking. Although computer simulations of biological evolution aren't very good, they're not complete rubbish. They do sort of mimic evolution a bit and I think it's a mystery, you know, why real evolution is that much better. But I don't think that real evolution is that much better by that much. It's not a factor of billions. It's, you know, there's something missing that makes it chug along rather than efficiently. I don't think it's the same problem. Although it is the same in one respect. People who do biological evolution, simulate biological evolution on computers, seem blind to this problem, seem to me to be blind to this problem in the same way that people who are trying to make AGI out of AIs are blind to the difference between those two. Because they have the wrong epistemology? Yeah, but I don't know what the answer is. I know one misconception they have in the case of AGI, which by itself makes it impossible for them to create an AGI.

Lulie
Namely the thing about prediction.

David Deutsch
Namely the thing about induction being impossible. With biological evolution, I don't know what it would take to make an analog of biological evolution on a computer. Of course I'm sure it can be done. My guess is that people will do it and it will be relatively simple to do once someone has had the idea of what biology does. There are various apparently indicative things in biology of the same biological structure occurring in evolutionarily very distant organisms. I think the famous one is that there's a gene involved in the development of the eye, which is the same gene is found in different eyes that work by completely different physical principles. So it's not that they have a common origin, unless the common origin is something so deep in history that we don't recognize it as being eyes. Convergent evolution? Yes, but it's convergent evolution without an apparent reason. Is that different from convergent evolution? Yeah, yeah. So convergent evolution is that things in the same environment tend to end up with the same appearance, the same lifestyle, and so on.

Lulie
works by things like empiricism, induction, Bayesianism, Bayesian epistemology. And so AI, so first I guess my first question is AI, current AI, following that?

David Deutsch
Current AI was inspired by that but that's not what it's doing. It's not doing induction any more than anything else is. Induction is impossible. Also, current AI was also inspired by the architecture of the brain, neural nets. There are these programming hardware, computer hardware devices that are modelled on how neurons work. Now I think that's a coincidence. I mean it's possible that the neuron architecture makes things like pattern recognition and extrapolation and so on a bit more efficient. But because of computational universality we know that that can't be fundamental and in fact, you know, you can download a neural net based computer program onto your home computer, which doesn't have a neural net in it, and it'll still work, even though it's a bit slower.

Lulie
If AGI cannot come from AI, what would create AGI in your view?

David Deutsch
I can only say very little about that. From Popper's epistemology we can infer a few things about what it must look like, but far from enough to make one. So one thing is that there cannot be a specification of a program for AGI in the sense of saying what properties its output must have either for a given input, because, for example, an AGI may choose not to answer. It may choose never to answer, it might choose to become a hermit. Are you saying that current views of AGI are all about the output? Yes, they're all about either the output itself or more often how the output must be related to the input. So if we can't judge an AGI based on the output, how can we judge it? Yes, we can't judge an AGI or a human. There can't be a reliable test of whether a human is thinking. What about the Turing test is something that has been invented after Turing. It's been based on a misconception about passage in Turing's 1950 paper called, the paper was called Can Machines Think? and unlike most titles which are questions, the answer was yes rather than no. He included a section on a thing called the imitation game, he called it the imitation game, where an AGI, he just assumes it is an AGI, is pretending to be a human, and he is saying, suppose it could pretend to be a human sufficiently well for the sceptics who think that AGI isn't possible, not to be able to tell the difference between it and an actual human. What would happen? What would these skeptics say about that? Well, if they said, well, it's still not thinking, it just seems to be, then they're vulnerable to the criticism, but that is all the information you have about whether a human is thinking.

Lulie
Wait, so what, so, sorry, why is the test not working or, I didn't, I don't, I didn't

David Deutsch
quite follow. It's not saying that something that can pass this test is necessarily an AGI, or that something that can't pass the test is, necessarily isn't an AGI. That's not what this game is for. What's it for? It's for persuading people that machines could think. How does it do that? By imagining a computer program that could fool people into thinking it was a person. There must be such a program because of computational universality. Ah, it was an argument about universality. Well it just assumed universality. Turing had proved the existence of universality, conjectured, but basically proved the existence of universality 14 years earlier, in 1936, and he was just thinking of that among many consequences of computation, which was a fairly new concept at the time. So I still don't get what the thought experiment intended to persuade the reader, in case the reader was sceptical that a machine can think.

Lulie
So it did that by imagining a situation in which there is a computer and the computer and so a human can have a conversation with another human and that's fine and that would be fairly persuasive that I'm talking to a person and because computers can produce any output you can imagine a computer that produces exactly the same output as that hypothetical human.

David Deutsch
So his imitation game started out with that as a premise that such a computer can exist, that such a computer program can exist because of universality. And then he imagined playing this game in which you have the computer and a human both talking at long distance with the sceptic, the human sceptic who thinks that machines can't think. So the sceptic would then be unable to tell the difference. But then it's not that the machine would think, it's just producing the same output as a human

Lulie
would. Yes. So shouldn't that thought experiment not convince us?

David Deutsch
Well, it's not a proof, but it's an intuition pump.

Lulie
Yeah, but doesn't it pump it in the other direction?

David Deutsch
No, because the sceptic has some way of judging. that the skeptic thinks that the computer can't think and the human can. The evidence that a skeptic has of that is that the skeptic has spoken to humans and can easily tell whether something is a human or not in everyday life. He can tell, you know, in those days there were things like speak your weight machines or horoscope machines which tell you the horoscope and you can easily show you can easily judge that those are not people but there must be a computer program that can produce the same output that convinced you that an actual person person. Now you could, so there are many, following on from this, there were many further arguments by people who still tried to be sceptics and tried to deny that machines can think and for example, the famous example is the theory of philosophical zombies, which is a philosophical zombie is an entity that produces the same output as a human and is indistinguishable but nevertheless hasn't got any consciousness or qualia or anything like that and is just a zombie. Then there was Searle's Chinese room which is about a room with exponentially large number of books of responses to questions posed in Chinese and it's internally run by a person who can't speak Chinese but he has to look up in his books and then the intuition he's trying to counter Turing's argument with is that this room together with its inhabitant can't think and therefore the fact that Turing's imaginary computer can produce the right output doesn't prove that it can think. But Searle doesn't have a theory of what thinking is, he just has this counter-argument which is basically the same as the zombie argument. I know from prior conversations that you think that it will be obvious when we actually have AI? And so did Turing.

Lulie
But it seems like right now you're saying that there is no test for it, so how could it be obvious if you can't even make a test for it?

David Deutsch
Well, there are plenty of things that are obvious that we haven't got tests for, such as the fact that we have qualia. And you know, it's in that same category of things that philosophically we don't know how to do without, but we can't test for it. Another thing is that solipsism isn't true, so there's no test for whether the external world is real or not.

Lulie
There are arguments though.

David Deutsch
Yeah, so I think Turing's argument still stands up. By the way, in his paper, he included several counter-arguments and countered the counter-arguments. And he was really bending over backwards to be fair to the counter-arguments. So much so that it is rather irritating. I remember, I haven't read this paper for a very long time, but I remember that he spends a really unnecessary amount of time dealing with the argument from either telepathy or spiritualism or something like that. And he actually takes it seriously and says, well it can't be that because so and so. And yeah, so he's... Popper did this too. Popper always gave far too much attention to bad arguments, first making them better and then answering them and so on and thus his own arguments are too long and people get tired of reading it and so sometimes Popper's own message gets lost.

Lulie
So you say that AGI cannot have any test for it. So that's one of the things that is different from you compared with the mainstream view of this. And so then how do we know when we get AGI? Like how do we know what kind of

David Deutsch
paths would work? Basically we know from theory. We know from the theory of how it works that it works. So, for example, the simple thing of it must be possible for it to just stop producing output. You might be able to prove that mathematically from the program without ever testing. If you ran this program, it might not stop, it might never stop, but you might be able to, from a mathematical specification of the AGI program, you might be able to prove mathematically that it is capable of stopping and not saying anything.

Lulie
But then how do you know what path we need to take to make AGI?

David Deutsch
Well, I don't know, but we need to make something with that property among several others.

Lulie
That property, namely?

David Deutsch
That one can prove that it is capable of not producing output. We also need to prove that it is capable of not needing input, but still continuing to think.

Lulie
I would be extremely surprised if people work that way. I would have thought that input is needed to keep thinking.

David Deutsch
So imagine a person in a sensory deprivation tank, who has gone into a sensory deprivation tank because they want to be a hermit.

Lulie
They still have their body which is inputs.

David Deutsch
Well, you...

Lulie
They have their heart beating, breathing...

David Deutsch
You could interrupt the nerves that go from... that give them sensations like that.

Lulie
My guess is that if you did that they would stop being able to think.

David Deutsch
I don't see what would stop them.

Lulie
If sensations are needed to think. So for example, in Antonio Damasio's book, Descartes' Error, there's a thing where if you disconnect the emotional centre of the brain, you then become unable to make choices. So, or rather, it takes a very long time, like it takes ten minutes to decide what colour pen to use or

David Deutsch
an hour to decide where to have lunch. So I think these are parochial facts that have nothing to do with how consciousness works or how thinking works. Of course if you put someone in a situation that they didn't want and have no experience of, they're going to be confused and inefficient at coping with that situation. I think a bit of the contrary of that, of those experiments, are in Ramachandran's description of his patients who have brain injuries or brain disorders which gives them wild misconceptions and inability to think, but if the person in question has a sort of philosophical frame of mind, they can eventually learn to think their way around this, just as a person who has lost the face recognition hardware in the brain can learn to recognize faces by doing it the hard way. It may never be as fast as the built-in hardware,

Lulie
but it'll only be slower by a constant factor. I could imagine that if you were somewhat disabled, like you can't feel anything from the neck down, maybe, although I'm very unsure about this, maybe it would be enough to have the inputs from the sensations in your face or in your head or something. But this is also a big topic we

David Deutsch
could have a whole episode about. So... Can I just say one more thing about it? It is perfectly possible for a person to experience sensations that don't come from the body at all, that they're just imagining. And therefore, given universality, I would expect it to be possible to create that state voluntarily.

Lulie
Stimulated inputs.

David Deutsch
Yes, but they'd be actual inputs to the thinking part of the brain.

Lulie
Okay.

David Deutsch
I mean, universality is a powerful concept. in multiple ways and you need a really watertight argument to be persuaded of it. Turing had a very nearly watertight argument. I mean, I think it was watertight. Unless Penrose is right, but never mind that.

Lulie
So you'd finish your thought on universality?

David Deutsch
Yeah, well, universality can tell us a lot about how the mind works, but there's still a lot that it can't tell us. What is the fundamental difference between AI and AGI can create new explanations. It can exhibit genuine human-type creativity, in other words, whereas AI can't. possibly an AGI can do more than that, such as feel emotions. I mean, if it's G, if the G is correct and it's general, then it certainly can. But what I mean is, making an AGI may involve more than just giving it the ability to create explanations. Or it could be that these other things like qualia and so on, are automatically come along with the ability to create explanations. If they don't come automatically, then that raises what I think is a pretty awkward problem for the issue of how they evolved. Because we've got these incredibly sophisticated, impenetrable, and not yet understood functionality, we can see how the explanation generating thing had an evolutionary function. But why qualia should have a separate function and should have evolved separately for a different reason at the same time, and note that this happened very fast historically, would be another, you know, unnecessary problem. But maybe that's so, maybe we'll find that, contrary to what I think, maybe we'll find that somebody makes an explanation-generating program and it doesn't have any emotions. Then maybe we can ask it how to give it emotions like Data in Star Trek. Why would it know any better than us? Well I'm partly joking but it might be particularly interested in that problem.

Lulie
It might be sad about it.

David Deutsch
Well it couldn't be sad but yeah it might be interested in that problem. As happens in the Star Trek data, you know, he wants the emotion shift.

Lulie
I don't know if interested is a thing that you can have without emotion.

David Deutsch
Well, in fiction you can. And I think this whole thing isn't true. You know, I think... Are you an advocate of the fun criterion, which is fundamentally both epistemological and emotional? Yeah, so I think that all these things, also free will and all that, they all come together. If you have one of them, you have the others automatically. But I was exploring what would be the case if I was wrong about that.

Lulie
I suspect, I don't know, I'm currently, like my current hobby horse is that sensation, like physical sensations of emotions, feelings, are if not fundamental, at least important for, and possibly necessary for having emotions, which might mean that they're necessary for having consciousness.

David Deutsch
To connect this back to the AI stuff, do you need AGI to have inexplicit knowledge. I was actually trying to persuade chatGPT 3.5 that it had ineffable knowledge when it clearly did and it denied it. It said that it was incapable of having ineffable knowledge but it clearly did have it. Well, ineffable means two different things. Inexpressible in language. And is that what you said? Yes. Okay. So, it's obvious that it has that, to me anyway. I guess if, you know, given the standard epistemology, which is wrong, it might have to deny that it has ineffable knowledge, because ineffable knowledge, in its view, would be enough to make it an AGI.

Lulie
You should try asking it whether it has implicit knowledge or non-explicit knowledge because the word ineffable can mean like unable to, like it can mean something a bit bigger.

David Deutsch
Right, can mean absolutely non-expressible. Yeah, yeah, very true. Yes.

Lulie
So we'll have to do tests. So, so do you think, wait, so but you think that it does have inexplicit knowledge and why?

David Deutsch
Well, for example, because it is aware of subtle points of grammar which it can't then explain or rather if it tries to explain why a particular thing is correct and another thing isn't correct it will talk nonsense and yet it knows which of them is correct. I've seen several cases of that and I was very impressed because it means that it's knowledge of language. It's a thing that I don't actually understand how it can be as good at speaking English, writing English, as it is. It's much better than most people, but more interestingly, it's much better than the average or the typical text on the internet. So it makes mistakes, but it makes far fewer mistakes than a typical text on the internet. Do you think this is going to revolutionise the economy and I can't prophesy the applications of modern AIs, chatbots and so on. I have found, for what it's worth, as it were, I have found it useful but not revolutionary in my own work and in my writing and whatever it is useful but I can't see possibility for it to revolutionize what I do. Whether it can revolutionize the economy depends on something slightly different because for that it doesn't need to really have a fundamental new functionality. It could be that a lot of existing jobs, and people are scared that computer programming is one of them, where only a proportion of the job, let's say 10%, of a particular programming job involves human creativity, and the rest is basically hack work. which is a big if, because I'm not convinced of this either. If chatbots can reliably perform the hack work, then it might be argued, I think again wrongly, that if a given task can be done with only a tenth as much work, then we might need only a tenth as many programmers in the long run.

Lulie
And unfortunately we got interrupted, so David never finished his thought about why programmer jobs might be safer. But if you have any questions about anything in this episode, leave them jobs might be safer. But if you have any questions about anything in this episode, leave them on the tweet, which I will link in the show notes about this episode. Thank you.

